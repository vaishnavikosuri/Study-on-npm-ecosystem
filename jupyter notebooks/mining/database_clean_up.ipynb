{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAbVoPDmJKrR"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "055IeVdhRPRa",
        "outputId": "d7aae8b7-3131-4b6f-f693-b2b8a1fdcd2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydriller\n",
            "  Downloading PyDriller-2.6-py3-none-any.whl (33 kB)\n",
            "Collecting gitpython (from pydriller)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from pydriller) (2023.4)\n",
            "Requirement already satisfied: types-pytz in /usr/local/lib/python3.10/dist-packages (from pydriller) (2024.1.0.20240203)\n",
            "Collecting lizard (from pydriller)\n",
            "  Downloading lizard-1.17.10-py2.py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from gitpython->pydriller)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->pydriller)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: lizard, smmap, gitdb, gitpython, pydriller\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.42 lizard-1.17.10 pydriller-2.6 smmap-5.0.1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import requests\n",
        "from urllib.parse import urljoin\n",
        "import sqlite3\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "from google.colab import drive\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "!pip install pydriller\n",
        "from pydriller import Repository\n",
        "from urllib.parse import urlparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXIhQ2Nauqdv",
        "outputId": "a5345898-305b-4828-c694-3719ca707595"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "# change location as per your convenience\n",
        "# final_packages.txt (containing json dump of list of npm package names to be mined) should be present at this location\n",
        "# database will be saved at this location\n",
        "os.chdir(\"/content/gdrive/Shareddrives/ECS 260/final\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TkJKg7MJOpQ"
      },
      "source": [
        "# Function Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSziMK10yLPd"
      },
      "outputs": [],
      "source": [
        "def add_column_if_not_exists(cursor, table_name, column_name, column_definition):\n",
        "    # check if the column already exists\n",
        "    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
        "    existing_columns = [column[1] for column in cursor.fetchall()]\n",
        "\n",
        "    if column_name not in existing_columns:\n",
        "        # add the column if it does not exist\n",
        "        cursor.execute(f\"ALTER TABLE {table_name} ADD COLUMN {column_name} {column_definition};\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qY13_R3fJlS"
      },
      "outputs": [],
      "source": [
        "def GET(json_obj, keys, default = None):\n",
        "  if not isinstance(keys, list):\n",
        "    keys = [keys]\n",
        "  current = json_obj\n",
        "  try:\n",
        "    for key in keys:\n",
        "      if isinstance(current, list):\n",
        "        key = int(key)\n",
        "      current = current[key]\n",
        "    return current\n",
        "  except (TypeError, IndexError, KeyError):\n",
        "    return default\n",
        "\n",
        "def isValid(value):\n",
        "  if isinstance(value, list):\n",
        "    return not len(value) == 0\n",
        "  return value != None and value != \"\"\n",
        "\n",
        "def getN(arr):\n",
        "  return len(arr) if isinstance(arr, list) else None\n",
        "\n",
        "def isNumber(v):\n",
        "  return isinstance(v, int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5Th-HbpfRFN"
      },
      "outputs": [],
      "source": [
        "# fetch json from an API endpoint\n",
        "def fetch_response_without_fail(url, params = {}, headers = {}, shouldScrap = False, verbose = True):\n",
        "  retry_count = 0\n",
        "  max_retries = 15\n",
        "  time_interval = 10\n",
        "  status = \"\"\n",
        "  while retry_count < max_retries:\n",
        "    try:\n",
        "      response = requests.get(url, params=params, headers=headers)\n",
        "\n",
        "      if response.status_code == 200:\n",
        "        status = str(response.status_code)\n",
        "        if shouldScrap:\n",
        "          return (status, response)\n",
        "        else:\n",
        "          return (status, response.json())\n",
        "      elif response.status_code == 404:\n",
        "        status = str(response.status_code)\n",
        "        if verbose:\n",
        "          print(f\"Error: {response.status_code}, skipping url {url}\")\n",
        "        return (status, None)\n",
        "      elif response.status_code == 204:\n",
        "        status = str(response.status_code)\n",
        "        if verbose:\n",
        "          print(f\"Error: {response.status_code}, from url {url}\")\n",
        "        return (status, None)\n",
        "      else:\n",
        "        status = str(response.status_code)\n",
        "        if verbose:\n",
        "          print(f\"Error: {response.status_code}\")\n",
        "          print(params)\n",
        "        retry_count += 1\n",
        "        if verbose:\n",
        "          print(f\"Retrying in {time_interval} seconds... (Retry {retry_count}/{max_retries})\")\n",
        "        if response.status_code == 429:\n",
        "          retry_after = response.headers.get('Retry-After')\n",
        "          if retry_after:\n",
        "            if verbose:\n",
        "              print(\"Retrying after\", retry_after + 5, \"Seconds\")\n",
        "            time.sleep(int(retry_after) + 5)\n",
        "          else:\n",
        "            time.sleep(time_interval)\n",
        "        else:\n",
        "          time.sleep(time_interval)\n",
        "\n",
        "    except Exception as e:\n",
        "      status = f\"Exception occurred: {e}\"\n",
        "      if verbose:\n",
        "        print(status)\n",
        "      retry_count += 1\n",
        "      if verbose:\n",
        "        print(f\"Retrying in {time_interval} seconds... (Retry {retry_count}/{max_retries})\")\n",
        "      time.sleep(time_interval)\n",
        "\n",
        "  if verbose:\n",
        "    print(f\"Max retries reached. Unable to fetch data from {url}.\")\n",
        "  return (status, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzyVBE3N1R5F"
      },
      "outputs": [],
      "source": [
        "def get_lagging_dependencies_count(dependencies):\n",
        "  status = \"erred\"\n",
        "  if not isValid(dependencies):\n",
        "    return (status, None)\n",
        "\n",
        "  lagging_dependencies = 0\n",
        "  for dependency, specified_version in dependencies.items():\n",
        "    if specified_version == 'latest':\n",
        "      continue\n",
        "\n",
        "    (status, dependency_data) = fetch_response_without_fail(f'https://registry.npmjs.org/{dependency}/latest')\n",
        "    if not status == '200':\n",
        "      # skipping count if dependency doesn't exist for some reason\n",
        "      continue\n",
        "    latest_version = GET(dependency_data, 'version')\n",
        "    if latest_version and specified_version != latest_version:\n",
        "      lagging_dependencies += 1\n",
        "\n",
        "  return (\"success\", lagging_dependencies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUBwUbipGYs3"
      },
      "outputs": [],
      "source": [
        "def get_github_url(repo_url):\n",
        "  if not isValid(repo_url):\n",
        "    return None\n",
        "  repo_url_lowered = repo_url.lower()\n",
        "  if not repo_url_lowered.find('gist.github.com') == -1:\n",
        "    return None\n",
        "  if not repo_url_lowered.find('gitee.com') == -1:\n",
        "    return None\n",
        "\n",
        "  github_idx = repo_url_lowered.find('github.com')\n",
        "  if github_idx == -1:\n",
        "    if repo_url.endswith('.git'):\n",
        "      repo_url = repo_url[:-4]\n",
        "      if ':' in repo_url:\n",
        "        repo_url = repo_url.split(':')[-1]\n",
        "      path_segments = repo_url.split('/')\n",
        "      if len(path_segments) >= 2 and all(len(path) > 0 for path in path_segments[-2:]):\n",
        "        author_repo = \"/\".join(path_segments[-2:])\n",
        "\n",
        "        # checking for one special edge case\n",
        "        if 'training/gitify' in author_repo:\n",
        "          return None\n",
        "        return f\"https://github.com/{author_repo}\"\n",
        "      return None\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "  repo_url = repo_url[github_idx + len('github.com'):]\n",
        "  path_segments = repo_url.split('/')\n",
        "  if len(path_segments) >= 2 and all(len(path) > 0 for path in path_segments[-2:]):\n",
        "    author_repo = \"/\".join(path_segments[-2:])\n",
        "    author_repo = author_repo[:-4] if author_repo.endswith(\".git\") else author_repo\n",
        "    author_repo = author_repo[:author_repo.find('.git#')] if '.git#' in author_repo else author_repo\n",
        "\n",
        "    # checking for one special edge case\n",
        "    if 'salesforce-experience-platform-emu/luvio-next' in author_repo:\n",
        "      return None\n",
        "    return f\"https://github.com/{author_repo}\"\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def extract_github_url(repo_url):\n",
        "  if not isValid(repo_url):\n",
        "    return None\n",
        "  repo_url_lowered = repo_url.lower()\n",
        "  if not repo_url_lowered.find('gist.github.com') == -1:\n",
        "    return None\n",
        "  if not repo_url_lowered.find('gitee.com') == -1:\n",
        "    return None\n",
        "\n",
        "  github_idx = repo_url_lowered.find('github.com')\n",
        "  formed_url = get_github_url(repo_url)\n",
        "  if not repo_url_lowered.find('bitbucket') == -1 and github_idx == -1 and not formed_url == None:\n",
        "    # trying to fetch the formed url and see if it gives a valid repo\n",
        "    # as some bitbucket repos have moved to github and on coversion into\n",
        "    # github url gives a valid repository\n",
        "    (url_status, url_response) = fetch_response_without_fail(formed_url, shouldScrap=True, verbose=False)\n",
        "    if url_status == '200':\n",
        "      return formed_url\n",
        "    return None\n",
        "\n",
        "  return formed_url"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extract_github_url('git@bitbucket.org:Saartje87/blocks-v2.git')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "s39RELyZfd7h",
        "outputId": "c6a59346-eebb-4aea-c18f-efb015ea2188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://github.com/Saartje87/blocks-v2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dQiDlyt1RhAh",
        "outputId": "14c180db-6bb2-4433-fa3c-3627d2e632a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://github.com/joseprl89/BitbucketPRAnalysis'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "extract_github_url('git+ssh://git@github.com/joseprl89/BitbucketPRAnalysis.git#id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoT33POyPDIZ"
      },
      "outputs": [],
      "source": [
        "def extract_snyk_score(soup, category):\n",
        "    selector = f'.scores li span:-soup-contains(\"{category}\")'\n",
        "    category_element = soup.select_one(selector)\n",
        "\n",
        "    if category_element:\n",
        "        category_score = category_element.find_next('span', class_='vue--pill__body').get_text(strip=True)\n",
        "        return category_score.upper()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def scrap_snyk(package):\n",
        "  status = \"erred\"\n",
        "  try:\n",
        "    status, response = fetch_response_without_fail(f\"https://snyk.io/advisor/npm-package/{package}\", shouldScrap=True)\n",
        "    if not status == '200':\n",
        "      return (status, None)\n",
        "    response.raise_for_status() # Raise an HTTPError for bad responses\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    health_score_div = soup.select_one('.number span')\n",
        "    if not health_score_div:\n",
        "      if soup.find('div', class_='package-extra').find('p', class_='heading', string='This is a malicious package'):\n",
        "        # For malicious packages, health score is 0\n",
        "        health_score = 0\n",
        "      else:\n",
        "        # some error\n",
        "        status = f\"erred, element not found, status:{status}\"\n",
        "        health_score = None\n",
        "      return (status, {\n",
        "        \"health_score\": health_score,\n",
        "        \"security\": None,\n",
        "        \"popularity\": None,\n",
        "        \"maintenance\": None,\n",
        "        \"community\": None\n",
        "      })\n",
        "    health_score = health_score_div.get_text(strip=True).split('/')[0].strip()\n",
        "    try:\n",
        "      health_score = int(health_score)\n",
        "    except:\n",
        "      health_score\n",
        "\n",
        "    return (status, {\n",
        "      \"health_score\": health_score,\n",
        "      \"security\": extract_snyk_score(soup, 'security'),\n",
        "      \"popularity\": extract_snyk_score(soup, 'popularity'),\n",
        "      \"maintenance\": extract_snyk_score(soup, 'maintenance'),\n",
        "      \"community\": extract_snyk_score(soup, 'community')\n",
        "    })\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    status = str(response.status_code)\n",
        "    print(f\"Error scraping snyk for {package}: {e}\")\n",
        "\n",
        "  print(f\"Error scraping snyk for {package}\")\n",
        "  return (status, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLfJF5e7Uh8I"
      },
      "outputs": [],
      "source": [
        "def scrap_repo(repo_url):\n",
        "  status = \"erred\"\n",
        "  try:\n",
        "    status, response = fetch_response_without_fail(repo_url, shouldScrap=True)\n",
        "    if not status == '200':\n",
        "      return (status, None)\n",
        "    response.raise_for_status() # Raise an HTTPError for bad responses\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    border_grid_rows = soup.find_all(class_='BorderGrid-row')\n",
        "    contributors = None\n",
        "    for border_grid_row in border_grid_rows:\n",
        "      text = border_grid_row.text.strip()\n",
        "      if text.startswith('Contributors'):\n",
        "        span_element = border_grid_row.find('span')\n",
        "        if span_element:\n",
        "          span_text = span_element.get_text(strip=True).replace(\",\", \"\")\n",
        "          try:\n",
        "            contributors = int(span_text)\n",
        "          except:\n",
        "            contributors = span_text\n",
        "\n",
        "    commit_anchor_tag = soup.find(\"a\", class_=\"react-last-commit-history-group\")\n",
        "    no_of_commits = None\n",
        "    if commit_anchor_tag:\n",
        "      # Find all span elements within the anchor tag\n",
        "      spans = commit_anchor_tag.find_all(\"span\")\n",
        "      spans = spans[0].find_all(\"span\")\n",
        "\n",
        "      if len(spans) > 1:\n",
        "        commit_count_span = spans[1]\n",
        "        commit_count_text = commit_count_span.get_text(strip=True)\n",
        "        no_of_commits = int(''.join(filter(str.isdigit, commit_count_text)))\n",
        "\n",
        "    return (status, {\"contributors\": contributors, \"no_of_commits\": no_of_commits})\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    status = str(response.status_code)\n",
        "    print(f\"Error scraping github repo {repo_url}: {e}\")\n",
        "\n",
        "  print(f\"Error scraping github repo {repo_url}\")\n",
        "  return (status, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q-jm886gGwy"
      },
      "outputs": [],
      "source": [
        "def scrap_repo_net(repo_url):\n",
        "  status = \"erred\"\n",
        "  try:\n",
        "    status, response = fetch_response_without_fail(repo_url + '/network/dependents', shouldScrap=True)\n",
        "    if not status == '200':\n",
        "      return (status, None)\n",
        "    response.raise_for_status() # Raise an HTTPError for bad responses\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    btn_links = soup.find_all('a', class_='btn-link')\n",
        "    dependants_count = None\n",
        "    dependant_repos_count = None\n",
        "    for btn_link in btn_links:\n",
        "      text_content = btn_link.get_text(strip=True)\n",
        "\n",
        "      if \"Repositories\" in text_content:\n",
        "          dependants_count = int(''.join(filter(str.isdigit, (text_content.split()[0]))))\n",
        "      elif \"Packages\" in text_content:\n",
        "          dependant_repos_count = int(''.join(filter(str.isdigit, (text_content.split()[0]))))\n",
        "\n",
        "    return (status, {\"dependants_count\": dependants_count, \"dependant_repos_count\": dependant_repos_count})\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    status = str(response.status_code)\n",
        "    print(f\"Error scraping github repo {repo_url}: {e}\")\n",
        "\n",
        "  print(f\"Error scraping github repo {repo_url}\")\n",
        "  return (status, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSHVBUsigowJ"
      },
      "outputs": [],
      "source": [
        "def count_lines_of_code(repo_url):\n",
        "  total_lines_of_code = 0\n",
        "  filtered_lines_of_code = 0\n",
        "  file_set = set()\n",
        "  try:\n",
        "    # Iterate over commits in the repository\n",
        "    for commit in Repository(repo_url).traverse_commits():\n",
        "      # Iterate over modified files in each commit\n",
        "      for modified_file in commit.modified_files:\n",
        "        # Check if the file is a source code file\n",
        "        if modified_file.filename.endswith(('.py', '.json', '.md', '.html', '.yaml', '.yml', '.xml','.sh', '.css','.scss','.ts','.jsx','.tsx')):\n",
        "          filtered_lines_of_code = filtered_lines_of_code+ (modified_file.added_lines - modified_file.deleted_lines)\n",
        "\n",
        "        lines_of_code = modified_file.added_lines - modified_file.deleted_lines\n",
        "        file_set.add(modified_file.filename)\n",
        "        total_lines_of_code += lines_of_code\n",
        "  except Exception as e:\n",
        "    print(\"Didn't Work\", repo_url)\n",
        "    return None, None, None\n",
        "\n",
        "  return total_lines_of_code, filtered_lines_of_code, len(file_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-NQdxfMtX3N"
      },
      "outputs": [],
      "source": [
        "def fetch_package_data_from_db(db_path, package_name, table_name):\n",
        "  try:\n",
        "    curr_conn = sqlite3.connect(db_path)\n",
        "    curr_cursor = curr_conn.cursor()\n",
        "\n",
        "    curr_cursor.execute(f\"SELECT * FROM {table_name} WHERE package=?\", (package_name,))\n",
        "    curr_row = curr_cursor.fetchone()\n",
        "\n",
        "    curr_conn.close()\n",
        "\n",
        "    if curr_row:\n",
        "      column_names = [description[0] for description in curr_cursor.description]\n",
        "      package_data = dict(zip(column_names, curr_row))\n",
        "      return package_data\n",
        "    else:\n",
        "      return None\n",
        "  except:\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi63NiGtxDqI"
      },
      "source": [
        "# Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3XJDIoaKrsi",
        "outputId": "8528e26d-c17c-44aa-df64-74eaa7b851f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows present:  30000\n"
          ]
        }
      ],
      "source": [
        "gh_token = '<your_github_token>'\n",
        "li_api_key = '<your_libraries.io_token'\n",
        "\n",
        "with open('final_packages.txt', \"r\") as file:\n",
        "  final_packages = file.read()\n",
        "package_list = json.loads(final_packages)\n",
        "\n",
        "print(\"Total rows present: \", len(package_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kyxTPar_4kd"
      },
      "source": [
        "# DB Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZrKGN8K_Ew_"
      },
      "outputs": [],
      "source": [
        "# init database and cursor\n",
        "conn = sqlite3.connect(\"final_database.db\")\n",
        "cursor = conn.cursor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "id": "AHt0R-BPI1NN",
        "outputId": "db323357-97a0-48c5-8000-eb94f98eb011"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                            package npm_api_status latest_version  \\\n",
              "0         @gerrico/react-components            200         0.1.27   \n",
              "1      express-simple-app-generator            200          1.0.5   \n",
              "2                 generator-giraffe            200         1.5.11   \n",
              "3                   outdated-client            200          1.2.1   \n",
              "4      @semi-bot/semi-theme-shopify            200          0.2.2   \n",
              "...                             ...            ...            ...   \n",
              "29995                    haribotify            200          1.0.0   \n",
              "29996        eslint-config-sharecar            200          2.0.0   \n",
              "29997              webpack-to-ardoq            200          0.2.2   \n",
              "29998         zywave-content-search            200          0.0.6   \n",
              "29999                      ngdoc-md            200          1.0.3   \n",
              "\n",
              "       no_of_versions                                           keywords  \\\n",
              "0                27.0                                               None   \n",
              "1                 6.0                            [\"generate\", \"express\"]   \n",
              "2               143.0                               [\"yeoman-generator\"]   \n",
              "3                21.0                                               None   \n",
              "4                 1.0                             [\"semi-theme\", \"scss\"]   \n",
              "...               ...                                                ...   \n",
              "29995             4.0  [\"browserify\", \"html\", \"components\", \"browseri...   \n",
              "29996             5.0  [\"eslint\", \"eslintconfig\", \"config\", \"airbnb\",...   \n",
              "29997             3.0                                               None   \n",
              "29998             6.0                                               None   \n",
              "29999             4.0                       [\"ngdoc\", \"markdown\", \"cli\"]   \n",
              "\n",
              "       no_of_users  has_readme  has_homepage   last_modified_timestamp  \\\n",
              "0              NaN         1.0           1.0  2022-08-12T10:57:20.535Z   \n",
              "1              NaN         1.0           0.0  2023-09-26T09:25:54.330Z   \n",
              "2              NaN         1.0           1.0  2022-06-18T06:05:39.609Z   \n",
              "3              NaN         1.0           0.0  2022-05-12T09:34:06.984Z   \n",
              "4              NaN         1.0           0.0  2022-08-10T01:57:06.698Z   \n",
              "...            ...         ...           ...                       ...   \n",
              "29995          NaN         1.0           1.0  2022-06-18T19:22:16.094Z   \n",
              "29996          NaN         1.0           1.0  2022-06-17T20:06:55.433Z   \n",
              "29997          NaN         1.0           1.0  2023-10-11T07:55:07.798Z   \n",
              "29998          NaN         1.0           0.0  2022-05-25T18:01:53.677Z   \n",
              "29999          NaN         1.0           1.0  2022-06-21T07:13:18.009Z   \n",
              "\n",
              "              created_timestamp  ...                  security popularity  \\\n",
              "0      2022-08-07T04:17:08.603Z  ...    SECURITY REVIEW NEEDED    LIMITED   \n",
              "1      2023-09-20T09:08:30.306Z  ...  NO KNOWN SECURITY ISSUES    LIMITED   \n",
              "2      2013-10-22T13:10:45.431Z  ...    SECURITY REVIEW NEEDED    LIMITED   \n",
              "3      2019-07-08T02:37:37.234Z  ...    SECURITY REVIEW NEEDED    LIMITED   \n",
              "4      2022-08-10T01:57:06.319Z  ...  NO KNOWN SECURITY ISSUES    LIMITED   \n",
              "...                         ...  ...                       ...        ...   \n",
              "29995  2016-01-27T10:34:18.517Z  ...  NO KNOWN SECURITY ISSUES    LIMITED   \n",
              "29996  2017-04-20T07:37:46.942Z  ...  NO KNOWN SECURITY ISSUES    LIMITED   \n",
              "29997  2020-02-13T13:44:59.576Z  ...    SECURITY REVIEW NEEDED      SMALL   \n",
              "29998  2022-03-04T19:23:37.896Z  ...    SECURITY REVIEW NEEDED    LIMITED   \n",
              "29999  2015-08-31T02:02:19.293Z  ...    SECURITY REVIEW NEEDED    LIMITED   \n",
              "\n",
              "       maintenance  community  avg_monthly_download  \\\n",
              "0         INACTIVE    LIMITED                  14.0   \n",
              "1      SUSTAINABLE    LIMITED                   6.0   \n",
              "2         INACTIVE    LIMITED                 357.0   \n",
              "3         INACTIVE    LIMITED                  58.0   \n",
              "4         INACTIVE    LIMITED                   3.0   \n",
              "...            ...        ...                   ...   \n",
              "29995     INACTIVE    LIMITED                  23.0   \n",
              "29996     INACTIVE    LIMITED                  20.0   \n",
              "29997     INACTIVE    LIMITED                 800.0   \n",
              "29998     INACTIVE    LIMITED                  11.0   \n",
              "29999     INACTIVE    LIMITED                  15.0   \n",
              "\n",
              "      days_since_last_modification  security_numeric  popularity_numeric  \\\n",
              "0                            559.0              50.0                20.0   \n",
              "1                            149.0             100.0                20.0   \n",
              "2                            614.0              50.0                20.0   \n",
              "3                            651.0              50.0                20.0   \n",
              "4                            561.0             100.0                20.0   \n",
              "...                            ...               ...                 ...   \n",
              "29995                        614.0             100.0                20.0   \n",
              "29996                        615.0             100.0                20.0   \n",
              "29997                        134.0              50.0                10.0   \n",
              "29998                        638.0              50.0                20.0   \n",
              "29999                        611.0              50.0                20.0   \n",
              "\n",
              "       maintenance_numeric community_numeric  \n",
              "0                      5.0              20.0  \n",
              "1                     50.0              20.0  \n",
              "2                      5.0              20.0  \n",
              "3                      5.0              20.0  \n",
              "4                      5.0              20.0  \n",
              "...                    ...               ...  \n",
              "29995                  5.0              20.0  \n",
              "29996                  5.0              20.0  \n",
              "29997                  5.0              20.0  \n",
              "29998                  5.0              20.0  \n",
              "29999                  5.0              20.0  \n",
              "\n",
              "[30000 rows x 56 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0bdb67a3-2e4d-42f5-b9ee-890d75a0ea14\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>package</th>\n",
              "      <th>npm_api_status</th>\n",
              "      <th>latest_version</th>\n",
              "      <th>no_of_versions</th>\n",
              "      <th>keywords</th>\n",
              "      <th>no_of_users</th>\n",
              "      <th>has_readme</th>\n",
              "      <th>has_homepage</th>\n",
              "      <th>last_modified_timestamp</th>\n",
              "      <th>created_timestamp</th>\n",
              "      <th>...</th>\n",
              "      <th>security</th>\n",
              "      <th>popularity</th>\n",
              "      <th>maintenance</th>\n",
              "      <th>community</th>\n",
              "      <th>avg_monthly_download</th>\n",
              "      <th>days_since_last_modification</th>\n",
              "      <th>security_numeric</th>\n",
              "      <th>popularity_numeric</th>\n",
              "      <th>maintenance_numeric</th>\n",
              "      <th>community_numeric</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@gerrico/react-components</td>\n",
              "      <td>200</td>\n",
              "      <td>0.1.27</td>\n",
              "      <td>27.0</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2022-08-12T10:57:20.535Z</td>\n",
              "      <td>2022-08-07T04:17:08.603Z</td>\n",
              "      <td>...</td>\n",
              "      <td>SECURITY REVIEW NEEDED</td>\n",
              "      <td>LIMITED</td>\n",
              "      <td>INACTIVE</td>\n",
              "      <td>LIMITED</td>\n",
              "      <td>14.0</td>\n",
              "      <td>559.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>express-simple-app-generator</td>\n",
              "      <td>200</td>\n",
              "      <td>1.0.5</td>\n",
              "      <td>6.0</td>\n",
              "      <td>[\"generate\", \"express\"]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2023-09-26T09:25:54.330Z</td>\n",
              "      <td>2023-09-20T09:08:30.306Z</td>\n",
              "      <td>...</td>\n",
              "      <td>NO KNOWN SECURITY ISSUES</td>\n",
              "      <td>LIMITED</td>\n",
              "      <td>SUSTAINABLE</td>\n",
              "      <td>LIMITED</td>\n",
              "      <td>6.0</td>\n",
              "      <td>149.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>generator-giraffe</td>\n",
              "      <td>200</td>\n",
              "      <td>1.5.11</td>\n",
              "      <td>143.0</td>\n",
              "      <td>[\"yeoman-generator\"]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2022-06-18T06:05:39.609Z</td>\n",
              "      <td>2013-10-22T13:10:45.431Z</td>\n",
              "      <td>...</td>\n",
              "      <td>SECURITY REVIEW NEEDED</td>\n",
              "      <td>LIMITED</td>\n",
              "      <td>INACTIVE</td>\n",
              "      <td>LIMITED</td>\n",
              "      <td>357.0</td>\n",
              "      <td>614.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>outdated-client</td>\n",
              "      <td>200</td>\n",
              "      <td>1.2.1</td>\n",
              "      <td>21.0</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2022-05-12T09:34:06.984Z</td>\n",
              "      <td>2019-07-08T02:37:37.234Z</td>\n",
              "      <td>...</td>\n",
              "      <td>SECURITY REVIEW NEEDED</td>\n",
              "      <td>LIMITED</td>\n",
              "      <td>INACTIVE</td>\n",
              "      <td>LIMITED</td>\n",
              "      <td>58.0</td>\n",
              "      <td>651.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@semi-bot/semi-theme-shopify</td>\n",
              "      <td>200</td>\n",
              "      <td>0.2.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[\"semi-theme\", \"scss\"]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2022-08-10T01:57:06.698Z</td>\n",
              "      <td>2022-08-10T01:57:06.319Z</td>\n",
              "      <td>...</td>\n",
              "      <td>NO KNOWN SECURITY ISSUES</td>\n",
              "      <td>LIMITED</td>\n",
              "      <td>INACTIVE</td>\n",
              "      <td>LIMITED</td>\n",
              "      <td>3.0</td>\n",
              "      <td>561.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29995</th>\n",
              "      <td>haribotify</td>\n",
              "      <td>200</td>\n",
              "      <td>1.0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>[\"browserify\", \"html\", \"components\", \"browseri...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2022-06-18T19:22:16.094Z</td>\n",
              "      <td>2016-01-27T10:34:18.517Z</td>\n",
              "      <td>...</td>\n",
              "      <td>NO KNOWN SECURITY ISSUES</td>\n",
              "      <td>LIMITED</td>\n",
              "      <td>INACTIVE</td>\n",
              "      <td>LIMITED</td>\n",
              "      <td>23.0</td>\n",
              "      <td>614.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29996</th>\n",
              "      <td>eslint-config-sharecar</td>\n",
              "      <td>200</td>\n",
              "      <td>2.0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[\"eslint\", \"eslintconfig\", \"config\", \"airbnb\",...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2022-06-17T20:06:55.433Z</td>\n",
              "      <td>2017-04-20T07:37:46.942Z</td>\n",
              "      <td>...</td>\n",
              "      <td>NO KNOWN SECURITY ISSUES</td>\n",
              "      <td>LIMITED</td>\n",
              "      <td>INACTIVE</td>\n",
              "      <td>LIMITED</td>\n",
              "      <td>20.0</td>\n",
              "      <td>615.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29997</th>\n",
              "      <td>webpack-to-ardoq</td>\n",
              "      <td>200</td>\n",
              "      <td>0.2.2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2023-10-11T07:55:07.798Z</td>\n",
              "      <td>2020-02-13T13:44:59.576Z</td>\n",
              "      <td>...</td>\n",
              "      <td>SECURITY REVIEW NEEDED</td>\n",
              "      <td>SMALL</td>\n",
              "      <td>INACTIVE</td>\n",
              "      <td>LIMITED</td>\n",
              "      <td>800.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29998</th>\n",
              "      <td>zywave-content-search</td>\n",
              "      <td>200</td>\n",
              "      <td>0.0.6</td>\n",
              "      <td>6.0</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2022-05-25T18:01:53.677Z</td>\n",
              "      <td>2022-03-04T19:23:37.896Z</td>\n",
              "      <td>...</td>\n",
              "      <td>SECURITY REVIEW NEEDED</td>\n",
              "      <td>LIMITED</td>\n",
              "      <td>INACTIVE</td>\n",
              "      <td>LIMITED</td>\n",
              "      <td>11.0</td>\n",
              "      <td>638.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29999</th>\n",
              "      <td>ngdoc-md</td>\n",
              "      <td>200</td>\n",
              "      <td>1.0.3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>[\"ngdoc\", \"markdown\", \"cli\"]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2022-06-21T07:13:18.009Z</td>\n",
              "      <td>2015-08-31T02:02:19.293Z</td>\n",
              "      <td>...</td>\n",
              "      <td>SECURITY REVIEW NEEDED</td>\n",
              "      <td>LIMITED</td>\n",
              "      <td>INACTIVE</td>\n",
              "      <td>LIMITED</td>\n",
              "      <td>15.0</td>\n",
              "      <td>611.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30000 rows × 56 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0bdb67a3-2e4d-42f5-b9ee-890d75a0ea14')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0bdb67a3-2e4d-42f5-b9ee-890d75a0ea14 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0bdb67a3-2e4d-42f5-b9ee-890d75a0ea14');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2bfd5b43-108e-4a2a-9ddb-a87968399814\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2bfd5b43-108e-4a2a-9ddb-a87968399814')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2bfd5b43-108e-4a2a-9ddb-a87968399814 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "df = pd.read_sql_query(f\"SELECT * FROM package_data\", conn)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing for errors in snyk data and re-mining\n",
        "\n",
        "Certain values were scrapped as '?' despite having valid values on webpage.Hence scraping those packages again."
      ],
      "metadata": {
        "id": "knfu0leNEaxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cursor.execute(\"SELECT package FROM package_data WHERE (health_score IS NOT NULL OR security IS NOT NULL OR popularity IS NOT NULL OR maintenance IS NOT NULL OR community IS NOT NULL) AND snyk_scrapping_status = '404'\")\n",
        "rows_to_rescrap = cursor.fetchall()\n",
        "cursor.execute(\"SELECT package FROM package_data WHERE health_score = '0' AND (security IS NOT NULL OR popularity IS NOT NULL OR maintenance IS NOT NULL OR community IS NOT NULL)\")\n",
        "rows_to_rescrap += cursor.fetchall()\n",
        "cursor.execute(\"SELECT package FROM package_data WHERE (security IS NULL AND popularity IS NOT NULL)\")\n",
        "rows_to_rescrap += cursor.fetchall()\n",
        "cursor.execute(\"SELECT package FROM package_data WHERE (health_score = '?' OR security = '?' OR popularity = '?' OR maintenance = '?' OR community = '?') AND snyk_scrapping_status = '200'\")\n",
        "rows_to_rescrap += cursor.fetchall()\n",
        "packages_to_rescrap = [row[0] for row in rows_to_rescrap]\n",
        "\n",
        "print(f\"Rows to re-scrap: {len(packages_to_rescrap)}/{len(package_list)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-tyV93REUkX",
        "outputId": "5eddc069-a459-4079-de64-e194ecc25fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows to re-scrap: 1/30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-mining snyk data\n",
        "rows_re_scraped = 0\n",
        "for package in packages_to_rescrap:\n",
        "  final_data = {}\n",
        "\n",
        "  snyk_scrapping_status, snyk_scrapping_res = scrap_snyk(package)\n",
        "  final_data['snyk_scrapping_status'] = snyk_scrapping_status\n",
        "  print(f'Now mining for \\n\\t{package}\\n\\thttps://snyk.io/advisor/npm-package/{package}')\n",
        "\n",
        "  if snyk_scrapping_status == '200':\n",
        "    final_data['health_score'] = snyk_scrapping_res['health_score']\n",
        "    final_data['security'] = snyk_scrapping_res['security']\n",
        "    final_data['popularity'] = snyk_scrapping_res['popularity']\n",
        "    final_data['maintenance'] = snyk_scrapping_res['maintenance']\n",
        "    final_data['community'] = snyk_scrapping_res['community']\n",
        "\n",
        "    print(\"snyk.io API mined\")\n",
        "  else:\n",
        "    final_data['health_score'] = final_data['security'] = final_data['popularity'] = final_data['maintenance'] = final_data['community'] = None\n",
        "    print(f'Failed for {package}')\n",
        "\n",
        "  if len(final_data) > 0:\n",
        "    print(final_data)\n",
        "    print()\n",
        "    updates = [(column, value) for column, value in final_data.items()]\n",
        "\n",
        "    # Execute the UPDATE statement\n",
        "    update_statement = f\"UPDATE package_data SET {', '.join([f'{col} = ?' for col, _ in updates])} WHERE package = ?\"\n",
        "    cursor.execute(update_statement, [val for _, val in updates] + [package])\n",
        "\n",
        "    # Commit the changes and close the database connection\n",
        "    conn.commit()\n",
        "\n",
        "  rows_re_scraped += 1\n",
        "  print(\"--------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "  print(f\"Rows re-mined so far: {rows_re_scraped}/{len(packages_to_rescrap)}\")\n",
        "  print(\"--------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E79SPZDsFWnG",
        "outputId": "8265f588-a21e-4086-c061-2ab9a8b32b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now mining for \n",
            "\tkuda-delsia-p2p-binance-axxz5zf0v\n",
            "\thttps://snyk.io/advisor/npm-package/kuda-delsia-p2p-binance-axxz5zf0v\n",
            "snyk.io API mined\n",
            "{'snyk_scrapping_status': '200', 'health_score': 20, 'security': '?', 'popularity': 'LIMITED', 'maintenance': 'INACTIVE', 'community': 'LIMITED'}\n",
            "\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 1/1\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQkSHf9cLU_E"
      },
      "source": [
        "# Testing for errors in github url extraction and re-mining\n",
        "\n",
        "Certain urls were extracted as `None` due to error in `extract_github_url` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EGuDh6SLX1w",
        "outputId": "eea61710-b6f1-4100-db7c-98d00923d473"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows to re-mine: 0/30000\n",
            "Rows not to re-mine: 0/30000\n"
          ]
        }
      ],
      "source": [
        "cursor.execute(\"SELECT package, repository_url, git_repository_url, git_repository_status, git_repository_url_final FROM package_data\")\n",
        "rows = cursor.fetchall()\n",
        "\n",
        "packages_to_update = []\n",
        "rows_not_to_mine = []\n",
        "old_rows = []\n",
        "for row in rows:\n",
        "  package, repository_url, git_repository_url, git_repository_url_status, git_repository_url_final = row\n",
        "\n",
        "  # Calculate the new extracted url\n",
        "  calculated_url = extract_github_url(repository_url)\n",
        "\n",
        "  # Check if the calculated url is different from the extracted url\n",
        "  if git_repository_url != calculated_url:\n",
        "    packages_to_update.append((package, calculated_url, repository_url))\n",
        "    old_rows.append((git_repository_url, repository_url))\n",
        "    if not isValid(calculated_url):\n",
        "      rows_not_to_mine.append(package)\n",
        "  if not isValid(git_repository_url_final) and git_repository_url_status == '200':\n",
        "    packages_to_update.append((package, calculated_url, repository_url))\n",
        "    old_rows.append((git_repository_url, repository_url))\n",
        "    if not isValid(calculated_url):\n",
        "      rows_not_to_mine.append(package)\n",
        "  if isValid(git_repository_url_final) and git_repository_url_status == '404':\n",
        "    packages_to_update.append((package, calculated_url, repository_url))\n",
        "    old_rows.append((git_repository_url, repository_url))\n",
        "    if not isValid(calculated_url):\n",
        "      rows_not_to_mine.append(package)\n",
        "\n",
        "print(f\"Rows to re-mine: {len(packages_to_update)}/{len(package_list)}\")\n",
        "print(f\"Rows not to re-mine: {len(rows_not_to_mine)}/{len(package_list)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "packages_to_update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4ua6763vNLd",
        "outputId": "908149d5-1502-4674-f4f9-3472e16ce05d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn7wpGL1OdS6"
      },
      "outputs": [],
      "source": [
        "# Re-mining github data\n",
        "rows_done = 0\n",
        "for package, git_repository_url, repository_url in packages_to_update:\n",
        "  # mining github API data\n",
        "  git_repository_url_final = None\n",
        "  final_data = {'git_repository_url': git_repository_url, 'git_repository_url_final': git_repository_url_final, 'git_repository_status': None}\n",
        "  continue_mining = isValid(git_repository_url)\n",
        "\n",
        "  print(f\"Now checking {git_repository_url}\\n\\tfor {package}\\n\\tfrom {repository_url}\")\n",
        "  if continue_mining:\n",
        "    # getting redirected gh url\n",
        "    try:\n",
        "      git_repository_url_final = requests.get(git_repository_url, allow_redirects=False).headers['Location']\n",
        "    except Exception as e:\n",
        "      git_repository_url_final = git_repository_url\n",
        "\n",
        "    # checking if url is valid\n",
        "    try:\n",
        "      res = requests.get(git_repository_url_final)\n",
        "      git_repository_status = str(res.status_code)\n",
        "    except Exception as e:\n",
        "      git_repository_status = f'Exception: {e}'\n",
        "    final_data['git_repository_status'] = git_repository_status\n",
        "    continue_mining = git_repository_status == '200'\n",
        "\n",
        "  # continue github mining\n",
        "  if continue_mining:\n",
        "    print(\"Continuing...\", git_repository_url_final)\n",
        "    final_data['git_repository_url_final'] = git_repository_url_final\n",
        "\n",
        "    headers = {\n",
        "      'Authorization': f'token {gh_token}',\n",
        "      'Accept': 'application/vnd.github.v3+json'\n",
        "    }\n",
        "\n",
        "    repo_owner, repo_name = git_repository_url_final.split('/')[-2:]\n",
        "    gh_api_url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}\"\n",
        "\n",
        "    # mining github API data\n",
        "    gh_api_status, gh_api_res = fetch_response_without_fail(gh_api_url, headers=headers)\n",
        "    final_data['gh_api_status'] = gh_api_status\n",
        "    if gh_api_status == '200':\n",
        "      forks = GET(gh_api_res, 'forks_count')\n",
        "      final_data['forks'] = forks\n",
        "\n",
        "      stars = GET(gh_api_res, 'stargazers_count')\n",
        "      final_data['stars'] = stars\n",
        "\n",
        "      watchers = GET(gh_api_res, 'subscribers_count')\n",
        "      final_data['watchers'] = watchers\n",
        "\n",
        "      start_date_timestamp = GET(gh_api_res, 'created_at')\n",
        "      if isValid(start_date_timestamp):\n",
        "        start_date = datetime.strptime(start_date_timestamp, \"%Y-%m-%dT%H:%M:%SZ\").date()\n",
        "        # age till Feb 23, 2024\n",
        "        end_date = datetime(2024, 2, 23).date()\n",
        "        total_days = (end_date - start_date).days\n",
        "      else:\n",
        "        total_days = None\n",
        "\n",
        "\n",
        "    # mining github issues API data\n",
        "    gh_issues_api_url = f\"https://api.github.com/search/issues?q=is:issue%20is:open%20repo:{repo_owner}/{repo_name}\"\n",
        "    gh_issue_api_status, gh_issues_api_res = fetch_response_without_fail(gh_issues_api_url, headers={'Authorization': f'token {gh_token}'})\n",
        "    final_data['gh_issue_api_status'] = gh_issue_api_status\n",
        "    if gh_issue_api_status == '200':\n",
        "      issues = GET(gh_issues_api_res, 'total_count')\n",
        "      final_data['issues'] = issues\n",
        "\n",
        "\n",
        "    # mining github pr API data\n",
        "    gh_pr_api_url = f\"https://api.github.com/search/issues?q=is:pr%20is:open%20repo:{repo_owner}/{repo_name}\"\n",
        "    gh_pr_api_status, gh_pr_api_res = fetch_response_without_fail(gh_pr_api_url, headers={'Authorization': f'token {gh_token}'})\n",
        "    final_data['gh_pr_api_status'] = gh_pr_api_status\n",
        "    if gh_pr_api_status == '200':\n",
        "      pr = GET(gh_pr_api_res, 'total_count')\n",
        "      final_data['pr'] = pr\n",
        "\n",
        "\n",
        "    # scrapping repository data\n",
        "    gh_scrapping_status, gh_scrapping_res = scrap_repo(git_repository_url_final)\n",
        "    final_data['gh_scrapping_status'] = gh_scrapping_status\n",
        "\n",
        "    if gh_scrapping_status == '200':\n",
        "      final_data['contributors'] = gh_scrapping_res['contributors']\n",
        "      final_data['no_of_commits'] = gh_scrapping_res['no_of_commits']\n",
        "      final_data['avg_commit_freq'] = gh_scrapping_res['no_of_commits'] / total_days if isValid(total_days) and isValid(gh_scrapping_res['no_of_commits']) else None\n",
        "\n",
        "\n",
        "    # scrapping repository network data\n",
        "    gh_net_scrapping_status, gh_net_scrapping_res = scrap_repo_net(git_repository_url_final)\n",
        "    final_data['gh_net_scrapping_status'] = gh_net_scrapping_status\n",
        "\n",
        "    if gh_net_scrapping_status == '200':\n",
        "      final_data['dependants_count'] = gh_net_scrapping_res['dependants_count']\n",
        "      final_data['dependant_repos_count'] = gh_net_scrapping_res['dependant_repos_count']\n",
        "\n",
        "    # pydriller\n",
        "    try:\n",
        "      old_data = fetch_package_data_from_db('FINAL_PACKAGE_DATA.db', package, 'final_package_data')\n",
        "      if isValid(old_data) and (isValid(old_data['Lines Of Codes']) or isValid(old_data['Number of Files']) or isValid(old_data['Filtered Lines of Codes'])):\n",
        "        final_data['total_lines_of_code'] = old_data['Lines Of Codes']\n",
        "        final_data['filtered_lines_of_code'] = old_data['Number of Files']\n",
        "        final_data['no_of_files'] = old_data['Filtered Lines of Codes']\n",
        "      else:\n",
        "        # some repo urls have been same for multiple packages so fetching data from them instead of recalculating\n",
        "        cursor.execute(\"SELECT total_lines_of_code, filtered_lines_of_code, no_of_files FROM package_data WHERE git_repository_url_final = ? AND (total_lines_of_code IS NOT NULL OR filtered_lines_of_code IS NOT NULL OR no_of_files IS NOT NULL)\", [git_repository_url_final])\n",
        "        result_to_check = cursor.fetchone()\n",
        "        if isValid(result_to_check):\n",
        "          final_data['total_lines_of_code'], final_data['filtered_lines_of_code'], final_data['no_of_files'] = result_to_check\n",
        "        else:\n",
        "          # If the result is empty or values are None, calculate the lines of code\n",
        "          total_lines_of_code, filtered_lines_of_code, no_of_files = count_lines_of_code(git_repository_url_final)\n",
        "          final_data['total_lines_of_code'] = total_lines_of_code\n",
        "          final_data['filtered_lines_of_code'] = filtered_lines_of_code\n",
        "          final_data['no_of_files'] = no_of_files\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"pydriller failed for {package}\\n{e}\")\n",
        "  else:\n",
        "    print(\"Skipping...\", git_repository_url_final)\n",
        "\n",
        "  if len(final_data) > 0:\n",
        "    updates = [(column, value) for column, value in final_data.items()]\n",
        "\n",
        "    # Execute the UPDATE statement\n",
        "    update_statement = f\"UPDATE package_data SET {', '.join([f'{col} = ?' for col, _ in updates])} WHERE package = ?\"\n",
        "    cursor.execute(update_statement, [val for _, val in updates] + [package])\n",
        "\n",
        "    # Commit the changes and close the database connection\n",
        "    conn.commit()\n",
        "\n",
        "  rows_done += 1\n",
        "  print(\"--------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "  print(f\"Rows done so far: {rows_done}/{len(packages_to_update)}\")\n",
        "  print(\"--------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AkqpdUsQQeO"
      },
      "outputs": [],
      "source": [
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing for errors in contributors count and re-mining\n",
        "\n",
        "Certain urls were contributors count were extracted as `None` despite having valid github urls as they dont have contributors section on their repository page. Hence fetching the api to get contributors count, assuming its less than 30."
      ],
      "metadata": {
        "id": "S5Ye9KEMYgpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cursor.execute(\"SELECT package, git_repository_url_final, gh_scrapping_status FROM package_data WHERE git_repository_url_final IS NOT NULL AND contributors IS NULL\")\n",
        "rows_to_remine = cursor.fetchall()\n",
        "\n",
        "packages_to_remine = [(package, git_repository_url_final) for (package, git_repository_url_final, gh_scrapping_status) in rows_to_remine]\n",
        "\n",
        "print(f\"Rows to re-mine: {len(packages_to_remine)}/{len(package_list)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b659916e-5fab-4f86-c869-8b27e291b2cc",
        "id": "Kkq9vCNCYgqH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows to re-mine: 45/30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-mining github data\n",
        "rows_re_mined = 0\n",
        "for package, git_repository_url_final in packages_to_remine:\n",
        "  final_data = {}\n",
        "\n",
        "  headers = {\n",
        "    'Authorization': f'token {gh_token}',\n",
        "    'Accept': 'application/vnd.github.v3+json'\n",
        "  }\n",
        "\n",
        "  repo_owner, repo_name = git_repository_url_final.split('/')[-2:]\n",
        "  gh_contributors_api_url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contributors\"\n",
        "  print(f'Now mining for \\n\\t{package}\\n\\t{git_repository_url_final}\\n\\t{gh_contributors_api_url}')\n",
        "\n",
        "  gh_contributors_api_status, gh_contributors_api_res = fetch_response_without_fail(gh_contributors_api_url, headers=headers)\n",
        "  if gh_contributors_api_status == '200':\n",
        "    contributors = getN(gh_contributors_api_res)\n",
        "    if isValid(contributors) and contributors < 30:\n",
        "      final_data['contributors'] = contributors\n",
        "      print('Contributors mined! -->', contributors)\n",
        "    else:\n",
        "      # fetching contributors recursively through api\n",
        "      curr_contributors_count = 0\n",
        "      curr_page = 1\n",
        "      params = {'page': curr_page}\n",
        "      while contributors != 0:\n",
        "        curr_contributors_count += contributors\n",
        "        curr_page += 1\n",
        "        params = {'page': curr_page}\n",
        "        gh_contributors_api_status, gh_contributors_api_res = fetch_response_without_fail(gh_contributors_api_url, params=params, headers=headers)\n",
        "        contributors = getN(gh_contributors_api_res)\n",
        "      final_data['contributors'] = curr_contributors_count\n",
        "      print('Recursively contributors mined! -->', curr_contributors_count)\n",
        "  elif gh_contributors_api_status == '204':\n",
        "    scrap_status, scrap_res = fetch_response_without_fail(f'{git_repository_url_final}/graphs/contributors', shouldScrap=True)\n",
        "    if scrap_status == '200':\n",
        "      try:\n",
        "        soup = BeautifulSoup(scrap_res, 'html.parser')\n",
        "        contrib_person_elements = soup.find_all('li', class_='contrib-person')\n",
        "        final_data['contributors'] = len(contrib_person_elements)\n",
        "      except:\n",
        "        print('Failed further scraping')\n",
        "    else:\n",
        "      print(f'Failed for \\n\\t{git_repository_url_final}/graphs/contributors')\n",
        "  else:\n",
        "    print(f'Failed for \\n\\t{gh_contributors_api_url}\\n\\t{git_repository_url_final}')\n",
        "\n",
        "  if len(final_data) > 0:\n",
        "    updates = [(column, value) for column, value in final_data.items()]\n",
        "\n",
        "    # Execute the UPDATE statement\n",
        "    update_statement = f\"UPDATE package_data SET {', '.join([f'{col} = ?' for col, _ in updates])} WHERE package = ?\"\n",
        "    cursor.execute(update_statement, [val for _, val in updates] + [package])\n",
        "\n",
        "    # Commit the changes and close the database connection\n",
        "    conn.commit()\n",
        "\n",
        "  rows_re_mined += 1\n",
        "  print(\"--------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "  print(f\"Rows re-mined so far: {rows_re_mined}/{len(packages_to_remine)}\")\n",
        "  print(\"--------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c4f8e7-7afb-4feb-847f-aaaafb51570b",
        "id": "95b8-3vNYgqK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now mining for \n",
            "\tpratik-npm-package\n",
            "\thttps://github.com/PrimeOfC-star/pratik-npm-package\n",
            "\thttps://api.github.com/repos/PrimeOfC-star/pratik-npm-package/contributors\n",
            "Error: 204, from url https://api.github.com/repos/PrimeOfC-star/pratik-npm-package/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 1/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tformvader\n",
            "\thttps://github.com/tony5hot/formvader\n",
            "\thttps://api.github.com/repos/tony5hot/formvader/contributors\n",
            "Error: 204, from url https://api.github.com/repos/tony5hot/formvader/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 2/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tryutils\n",
            "\thttps://github.com/taglist/ryutils\n",
            "\thttps://api.github.com/repos/taglist/ryutils/contributors\n",
            "Error: 204, from url https://api.github.com/repos/taglist/ryutils/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 3/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tformula-gammaln\n",
            "\thttps://github.com/FormulaPages/gammaln\n",
            "\thttps://api.github.com/repos/FormulaPages/gammaln/contributors\n",
            "Error: 204, from url https://api.github.com/repos/FormulaPages/gammaln/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 4/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tcwasnippets\n",
            "\thttps://github.com/codewitham/CwaSnippet\n",
            "\thttps://api.github.com/repos/codewitham/CwaSnippet/contributors\n",
            "Error: 204, from url https://api.github.com/repos/codewitham/CwaSnippet/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 5/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tprettier-plugin-sort-classes\n",
            "\thttps://github.com/xin2014/prettier-plugin-sort-classes\n",
            "\thttps://api.github.com/repos/xin2014/prettier-plugin-sort-classes/contributors\n",
            "Error: 204, from url https://api.github.com/repos/xin2014/prettier-plugin-sort-classes/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 6/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tnick-login\n",
            "\thttps://github.com/yashp98/login-component\n",
            "\thttps://api.github.com/repos/yashp98/login-component/contributors\n",
            "Error: 204, from url https://api.github.com/repos/yashp98/login-component/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 7/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tcolter449mod\n",
            "\thttps://github.com/colter449/firstmod\n",
            "\thttps://api.github.com/repos/colter449/firstmod/contributors\n",
            "Error: 204, from url https://api.github.com/repos/colter449/firstmod/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 8/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tialib\n",
            "\thttps://github.com/monil-ia/npm\n",
            "\thttps://api.github.com/repos/monil-ia/npm/contributors\n",
            "Error: 204, from url https://api.github.com/repos/monil-ia/npm/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 9/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tformula-char\n",
            "\thttps://github.com/FormulaPages/char\n",
            "\thttps://api.github.com/repos/FormulaPages/char/contributors\n",
            "Error: 204, from url https://api.github.com/repos/FormulaPages/char/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 10/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tformula-transpose\n",
            "\thttps://github.com/FormulaPages/transpose\n",
            "\thttps://api.github.com/repos/FormulaPages/transpose/contributors\n",
            "Error: 204, from url https://api.github.com/repos/FormulaPages/transpose/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 11/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tnative-motion\n",
            "\thttps://github.com/aleksei-s-popov/capacitor-native-motion-plugi\n",
            "\thttps://api.github.com/repos/aleksei-s-popov/capacitor-native-motion-plugi/contributors\n",
            "Error: 204, from url https://api.github.com/repos/aleksei-s-popov/capacitor-native-motion-plugi/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 12/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\t@badcj/is-cjnpm\n",
            "\thttps://github.com/BADCJ/cj-npm-1\n",
            "\thttps://api.github.com/repos/BADCJ/cj-npm-1/contributors\n",
            "Error: 204, from url https://api.github.com/repos/BADCJ/cj-npm-1/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 13/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\t@pgchain/network-list\n",
            "\thttps://github.com/pgchain/network-list\n",
            "\thttps://api.github.com/repos/pgchain/network-list/contributors\n",
            "Error: 204, from url https://api.github.com/repos/pgchain/network-list/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 14/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\t@notealert/notealert\n",
            "\thttps://github.com/johnbabu021/notealert\n",
            "\thttps://api.github.com/repos/johnbabu021/notealert/contributors\n",
            "Error: 204, from url https://api.github.com/repos/johnbabu021/notealert/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 15/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tgo-juggle\n",
            "\thttps://github.com/scaperow/go-juggle\n",
            "\thttps://api.github.com/repos/scaperow/go-juggle/contributors\n",
            "Error: 204, from url https://api.github.com/repos/scaperow/go-juggle/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 16/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\t@abmao/stylelint-config-vue\n",
            "\thttps://github.com/hengshanMWC/stylelint-config\n",
            "\thttps://api.github.com/repos/hengshanMWC/stylelint-config/contributors\n",
            "Error: 204, from url https://api.github.com/repos/hengshanMWC/stylelint-config/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 17/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\t@padar/card\n",
            "\thttps://github.com/paziresh24/padar\n",
            "\thttps://api.github.com/repos/paziresh24/padar/contributors\n",
            "Error: 204, from url https://api.github.com/repos/paziresh24/padar/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 18/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tscrapgoo\n",
            "\thttps://github.com/marloncepeda/scrapgoo\n",
            "\thttps://api.github.com/repos/marloncepeda/scrapgoo/contributors\n",
            "Error: 204, from url https://api.github.com/repos/marloncepeda/scrapgoo/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 19/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tyuiezt_module\n",
            "\thttps://github.com/zhijiaxinyu/react_demo\n",
            "\thttps://api.github.com/repos/zhijiaxinyu/react_demo/contributors\n",
            "Error: 204, from url https://api.github.com/repos/zhijiaxinyu/react_demo/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 20/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tbradfrost-npm-test\n",
            "\thttps://github.com/bradfrost/npm-test\n",
            "\thttps://api.github.com/repos/bradfrost/npm-test/contributors\n",
            "Error: 204, from url https://api.github.com/repos/bradfrost/npm-test/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 21/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tqsy-vue-npm-test\n",
            "\thttps://github.com/Shruan/qsy-vue-npm-test\n",
            "\thttps://api.github.com/repos/Shruan/qsy-vue-npm-test/contributors\n",
            "Error: 204, from url https://api.github.com/repos/Shruan/qsy-vue-npm-test/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 22/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tvite-plugin-custom-functions-metadata\n",
            "\thttps://github.com/abrasher/vite-plugin-custom-functions-metadata\n",
            "\thttps://api.github.com/repos/abrasher/vite-plugin-custom-functions-metadata/contributors\n",
            "Error: 204, from url https://api.github.com/repos/abrasher/vite-plugin-custom-functions-metadata/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 23/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\treact-native-phantom-network\n",
            "\thttps://github.com/ColdJuzi/react-native-phantom-network\n",
            "\thttps://api.github.com/repos/ColdJuzi/react-native-phantom-network/contributors\n",
            "Error: 204, from url https://api.github.com/repos/ColdJuzi/react-native-phantom-network/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 24/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\txceling-configuration\n",
            "\thttps://github.com/ramaschneider/xceling\n",
            "\thttps://api.github.com/repos/ramaschneider/xceling/contributors\n",
            "Error: 204, from url https://api.github.com/repos/ramaschneider/xceling/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 25/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\t@pluxbox/pbx-prosemirror\n",
            "\thttps://github.com/pluxbox/pbx-prosemirror\n",
            "\thttps://api.github.com/repos/pluxbox/pbx-prosemirror/contributors\n",
            "Error: 204, from url https://api.github.com/repos/pluxbox/pbx-prosemirror/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 26/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\t@jowe81/lg\n",
            "\thttps://github.com/jowe81/lg\n",
            "\thttps://api.github.com/repos/jowe81/lg/contributors\n",
            "Error: 204, from url https://api.github.com/repos/jowe81/lg/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 27/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\twannado\n",
            "\thttps://github.com/cjg125/wannado\n",
            "\thttps://api.github.com/repos/cjg125/wannado/contributors\n",
            "Error: 204, from url https://api.github.com/repos/cjg125/wannado/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 28/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tvue3-drag-and-drop\n",
            "\thttps://github.com/KazumaChihaya/draganddrop\n",
            "\thttps://api.github.com/repos/KazumaChihaya/draganddrop/contributors\n",
            "Error: 204, from url https://api.github.com/repos/KazumaChihaya/draganddrop/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 29/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tcloud-config\n",
            "\thttps://github.com/k2wanko/node-cloud-config\n",
            "\thttps://api.github.com/repos/k2wanko/node-cloud-config/contributors\n",
            "Error: 204, from url https://api.github.com/repos/k2wanko/node-cloud-config/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 30/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\t@syncu-softwares/sd-react\n",
            "\thttps://github.com/syncu-softwares/sd-react\n",
            "\thttps://api.github.com/repos/syncu-softwares/sd-react/contributors\n",
            "Error: 204, from url https://api.github.com/repos/syncu-softwares/sd-react/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 31/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\thello-world-library\n",
            "\thttps://github.com/Anirudh-Konduri/sample-angular-library\n",
            "\thttps://api.github.com/repos/Anirudh-Konduri/sample-angular-library/contributors\n",
            "Error: 204, from url https://api.github.com/repos/Anirudh-Konduri/sample-angular-library/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 32/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\t@tobias.hellstrom.visma/npm-no-value\n",
            "\thttps://github.com/tobiashellstrom/npm-no-value\n",
            "\thttps://api.github.com/repos/tobiashellstrom/npm-no-value/contributors\n",
            "Error: 204, from url https://api.github.com/repos/tobiashellstrom/npm-no-value/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 33/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tformula-dollar\n",
            "\thttps://github.com/FormulaPages/dollar\n",
            "\thttps://api.github.com/repos/FormulaPages/dollar/contributors\n",
            "Error: 204, from url https://api.github.com/repos/FormulaPages/dollar/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 34/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tpj-dummypackage\n",
            "\thttps://github.com/Preshy-Jones/npm_package\n",
            "\thttps://api.github.com/repos/Preshy-Jones/npm_package/contributors\n",
            "Error: 204, from url https://api.github.com/repos/Preshy-Jones/npm_package/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 35/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\t@leocorbi/storybook-addon-extra-tab\n",
            "\thttps://github.com/LeonardoCorbi/extra-tab-storybook-addons\n",
            "\thttps://api.github.com/repos/LeonardoCorbi/extra-tab-storybook-addons/contributors\n",
            "Error: 204, from url https://api.github.com/repos/LeonardoCorbi/extra-tab-storybook-addons/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 36/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tramkapil\n",
            "\thttps://github.com/ram-yerra/Node\n",
            "\thttps://api.github.com/repos/ram-yerra/Node/contributors\n",
            "Error: 204, from url https://api.github.com/repos/ram-yerra/Node/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 37/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tformula-improduct\n",
            "\thttps://github.com/FormulaPages/improduct\n",
            "\thttps://api.github.com/repos/FormulaPages/improduct/contributors\n",
            "Error: 204, from url https://api.github.com/repos/FormulaPages/improduct/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 38/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\t@devsaur/release-it-bump-version-code\n",
            "\thttps://github.com/devsaur/release-it-bump-version-code\n",
            "\thttps://api.github.com/repos/devsaur/release-it-bump-version-code/contributors\n",
            "Error: 204, from url https://api.github.com/repos/devsaur/release-it-bump-version-code/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 39/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tlog-sdk-test\n",
            "\thttps://github.com/FionaCY/log-sdk\n",
            "\thttps://api.github.com/repos/FionaCY/log-sdk/contributors\n",
            "Error: 204, from url https://api.github.com/repos/FionaCY/log-sdk/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 40/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tjar-my-storybook-components\n",
            "\thttps://github.com/ChuyAR17/sb-components-test\n",
            "\thttps://api.github.com/repos/ChuyAR17/sb-components-test/contributors\n",
            "Error: 404, skipping url https://api.github.com/repos/ChuyAR17/sb-components-test/contributors\n",
            "Failed for \n",
            "\thttps://api.github.com/repos/ChuyAR17/sb-components-test/contributors\n",
            "\thttps://github.com/ChuyAR17/sb-components-test\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 41/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\toriorganization\n",
            "\thttps://github.com/vahidHossaini/oriorganization\n",
            "\thttps://api.github.com/repos/vahidHossaini/oriorganization/contributors\n",
            "Error: 204, from url https://api.github.com/repos/vahidHossaini/oriorganization/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 42/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\talert-capacitor\n",
            "\thttps://github.com/geyffer/alert-capacitor\n",
            "\thttps://api.github.com/repos/geyffer/alert-capacitor/contributors\n",
            "Error: 204, from url https://api.github.com/repos/geyffer/alert-capacitor/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 43/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\t@tarsiscruz/npm-helloworld\n",
            "\thttps://github.com/ctcruz/npm-helloword\n",
            "\thttps://api.github.com/repos/ctcruz/npm-helloword/contributors\n",
            "Error: 204, from url https://api.github.com/repos/ctcruz/npm-helloword/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 44/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Now mining for \n",
            "\tmyfirstlettertocapitalnpm\n",
            "\thttps://github.com/Minhal-Ahmed/npmPackage\n",
            "\thttps://api.github.com/repos/Minhal-Ahmed/npmPackage/contributors\n",
            "Error: 204, from url https://api.github.com/repos/Minhal-Ahmed/npmPackage/contributors\n",
            "Failed further scraping\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Rows re-mined so far: 45/45\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KBDqTKkN4xb"
      },
      "source": [
        "# Testing for errors in github issue and pr extraction and re-mining\n",
        "\n",
        "Certain github repo stats (eg. `issue`, `pr`, etc.) were not fetched, re-mining the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55583755-d2d7-48fe-8325-eb0be3004cfa",
        "id": "qnTY-lJnN4xc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows to re-mine: 0/30000\n"
          ]
        }
      ],
      "source": [
        "cursor.execute(\"SELECT package, git_repository_url, git_repository_url_final, repository_url FROM package_data WHERE git_repository_url_final IS NOT NULL AND (issues IS NULL OR pr IS NULL OR stars IS NULL OR forks IS NULL OR watchers IS NULL)\")\n",
        "packages_to_update = cursor.fetchall()\n",
        "cursor.execute(\"SELECT package, git_repository_url, git_repository_url_final, repository_url FROM package_data WHERE git_repository_url_final IS NULL AND (issues IS NOT NULL OR pr IS NOT NULL OR stars IS NOT NULL OR forks IS NOT NULL OR watchers IS NOT NULL)\")\n",
        "packages_to_update += cursor.fetchall()\n",
        "\n",
        "print(f\"Rows to re-mine: {len(packages_to_update)}/{len(package_list)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDLRjDCqN4xe"
      },
      "outputs": [],
      "source": [
        "# Re-mining github data\n",
        "rows_done = 0\n",
        "for package, git_repository_url_final in packages_to_update:\n",
        "  # mining github API data\n",
        "  final_data = {}\n",
        "\n",
        "  print(f\"Now checking {git_repository_url_final}\\n\\tfor {package}\")\n",
        "\n",
        "  headers = {\n",
        "    'Authorization': f'token {gh_token}',\n",
        "    'Accept': 'application/vnd.github.v3+json'\n",
        "  }\n",
        "\n",
        "  repo_owner, repo_name = git_repository_url_final.split('/')[-2:]\n",
        "  gh_api_url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}\"\n",
        "  print(f\"\\tforks, stars, watchers: {gh_api_url}\")\n",
        "\n",
        "  # mining github API data\n",
        "  gh_api_status, gh_api_res = fetch_response_without_fail(gh_api_url, headers=headers)\n",
        "  final_data['gh_api_status'] = gh_api_status\n",
        "  total_days = None\n",
        "  if gh_api_status == '200':\n",
        "    forks = GET(gh_api_res, 'forks_count')\n",
        "    final_data['forks'] = forks\n",
        "\n",
        "    stars = GET(gh_api_res, 'stargazers_count')\n",
        "    final_data['stars'] = stars\n",
        "\n",
        "    watchers = GET(gh_api_res, 'subscribers_count')\n",
        "    final_data['watchers'] = watchers\n",
        "\n",
        "    start_date_timestamp = GET(gh_api_res, 'created_at')\n",
        "    if isValid(start_date_timestamp):\n",
        "      start_date = datetime.strptime(start_date_timestamp, \"%Y-%m-%dT%H:%M:%SZ\").date()\n",
        "      # age till Feb 23, 2024\n",
        "      end_date = datetime(2024, 2, 23).date()\n",
        "      total_days = (end_date - start_date).days\n",
        "\n",
        "\n",
        "  # mining github issues API data\n",
        "  gh_issues_api_url = f\"https://api.github.com/search/issues?q=is:issue%20is:open%20repo:{repo_owner}/{repo_name}\"\n",
        "  print(f\"\\tissues: {gh_issues_api_url}\")\n",
        "  gh_issue_api_status, gh_issues_api_res = fetch_response_without_fail(gh_issues_api_url, headers={'Authorization': f'token {gh_token}'})\n",
        "  final_data['gh_issue_api_status'] = gh_issue_api_status\n",
        "  if gh_issue_api_status == '200':\n",
        "    issues = GET(gh_issues_api_res, 'total_count')\n",
        "    final_data['issues'] = issues\n",
        "\n",
        "\n",
        "  # mining github pr API data\n",
        "  gh_pr_api_url = f\"https://api.github.com/search/issues?q=is:pr%20is:open%20repo:{repo_owner}/{repo_name}\"\n",
        "  print(f\"\\tpr: {gh_pr_api_url}\")\n",
        "  gh_pr_api_status, gh_pr_api_res = fetch_response_without_fail(gh_pr_api_url, headers={'Authorization': f'token {gh_token}'})\n",
        "  final_data['gh_pr_api_status'] = gh_pr_api_status\n",
        "  if gh_pr_api_status == '200':\n",
        "    pr = GET(gh_pr_api_res, 'total_count')\n",
        "    final_data['pr'] = pr\n",
        "\n",
        "\n",
        "  # scrapping repository data\n",
        "  gh_scrapping_status, gh_scrapping_res = scrap_repo(git_repository_url_final)\n",
        "  final_data['gh_scrapping_status'] = gh_scrapping_status\n",
        "\n",
        "  if gh_scrapping_status == '200':\n",
        "    final_data['contributors'] = gh_scrapping_res['contributors']\n",
        "    final_data['no_of_commits'] = gh_scrapping_res['no_of_commits']\n",
        "    final_data['avg_commit_freq'] = gh_scrapping_res['no_of_commits'] / total_days if isValid(total_days) and isValid(gh_scrapping_res['no_of_commits']) else None\n",
        "\n",
        "  # scrapping repository network data\n",
        "  gh_net_scrapping_status, gh_net_scrapping_res = scrap_repo_net(git_repository_url_final)\n",
        "  final_data['gh_net_scrapping_status'] = gh_net_scrapping_status\n",
        "\n",
        "  if gh_net_scrapping_status == '200':\n",
        "    final_data['dependants_count'] = gh_net_scrapping_res['dependants_count']\n",
        "    final_data['dependant_repos_count'] = gh_net_scrapping_res['dependant_repos_count']\n",
        "\n",
        "  if len(final_data) > 0:\n",
        "    # updates = [(column, value) for column, value in final_data.items()]\n",
        "\n",
        "    # # Execute the UPDATE statement\n",
        "    # update_statement = f\"UPDATE package_data SET {', '.join([f'{col} = ?' for col, _ in updates])} WHERE package = ?\"\n",
        "    # cursor.execute(update_statement, [val for _, val in updates] + [package])\n",
        "\n",
        "    # # Commit the changes and close the database connection\n",
        "    # conn.commit()\n",
        "    print(final_data)\n",
        "\n",
        "  rows_done += 1\n",
        "  print(\"--------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "  print(f\"Rows done so far: {rows_done}/{len(packages_to_update)}\")\n",
        "  print(\"--------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kjbk69pgN4xe"
      },
      "outputs": [],
      "source": [
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdSaTfR7Gkn0"
      },
      "source": [
        "# Testing for errors in py-drilling\n",
        "\n",
        "Certain rows were skipped due to error in function `fetch_package_data_from_db` to fetch data from old database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjhaKJ92GkoF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd865e1f-0d9d-49d6-f304-75d881044b37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows to drill: 0/30000\n"
          ]
        }
      ],
      "source": [
        "cursor.execute(\"SELECT package, git_repository_url_final FROM package_data WHERE git_repository_url_final IS NOT NULL AND (total_lines_of_code IS NULL OR filtered_lines_of_code IS NULL OR no_of_files IS NULL)\")\n",
        "rows_to_drill = cursor.fetchall()\n",
        "cursor.execute(\"SELECT package, git_repository_url_final FROM package_data WHERE git_repository_url_final IS NULL AND (total_lines_of_code IS NOT NULL OR filtered_lines_of_code IS NOT NULL OR no_of_files IS NOT NULL)\")\n",
        "rows_to_drill += cursor.fetchall()\n",
        "\n",
        "packages_to_drill = [(package, git_repository_url_final) for (package, git_repository_url_final) in rows_to_drill]\n",
        "\n",
        "print(f\"Rows to drill: {len(packages_to_drill)}/{len(package_list)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1uP2JdzGkoJ"
      },
      "outputs": [],
      "source": [
        "# Re-mining github data\n",
        "rows_drilled = 0\n",
        "for package, git_repository_url_final in packages_to_drill:\n",
        "  final_data = {'total_lines_of_code': None, 'filtered_lines_of_code': None, 'no_of_files': None}\n",
        "\n",
        "  # pydrilling\n",
        "  print(f\"Now drilling {git_repository_url_final}\\n\\tfor {package}\")\n",
        "  try:\n",
        "    old_data = fetch_package_data_from_db('FINAL_PACKAGE_DATA.db', package, 'final_package_data')\n",
        "    if isValid(old_data) and (isValid(old_data['Lines Of Codes']) or isValid(old_data['Number of Files']) or isValid(old_data['Filtered Lines of Codes'])):\n",
        "      final_data['total_lines_of_code'] = old_data['Lines Of Codes']\n",
        "      final_data['filtered_lines_of_code'] = old_data['Number of Files']\n",
        "      final_data['no_of_files'] = old_data['Filtered Lines of Codes']\n",
        "    else:\n",
        "      # some repo urls have been same for multiple packages so fetching data from them instead of recalculating\n",
        "      cursor.execute(\"SELECT total_lines_of_code, filtered_lines_of_code, no_of_files FROM package_data WHERE git_repository_url_final = ? AND (total_lines_of_code IS NOT NULL OR filtered_lines_of_code IS NOT NULL OR no_of_files IS NOT NULL)\", [git_repository_url_final])\n",
        "      result_to_check = cursor.fetchone()\n",
        "      if isValid(result_to_check):\n",
        "        final_data['total_lines_of_code'], final_data['filtered_lines_of_code'], final_data['no_of_files'] = result_to_check\n",
        "      else:\n",
        "        # If the result is empty or values are None, calculate the lines of code\n",
        "        total_lines_of_code, filtered_lines_of_code, no_of_files = count_lines_of_code(git_repository_url_final)\n",
        "        final_data['total_lines_of_code'] = total_lines_of_code\n",
        "        final_data['filtered_lines_of_code'] = filtered_lines_of_code\n",
        "        final_data['no_of_files'] = no_of_files\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"pydriller failed for {package}\\n{e}\")\n",
        "\n",
        "  if len(final_data) > 0:\n",
        "    updates = [(column, value) for column, value in final_data.items()]\n",
        "\n",
        "    # Execute the UPDATE statement\n",
        "    update_statement = f\"UPDATE package_data SET {', '.join([f'{col} = ?' for col, _ in updates])} WHERE package = ?\"\n",
        "    cursor.execute(update_statement, [val for _, val in updates] + [package])\n",
        "\n",
        "    # Commit the changes and close the database connection\n",
        "    conn.commit()\n",
        "\n",
        "  rows_drilled += 1\n",
        "  print(\"--------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "  print(f\"Rows drilled so far: {rows_drilled}/{len(packages_to_drill)}\")\n",
        "  print(\"--------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# removing invalid values\n",
        "cursor.execute(\"UPDATE package_data SET sloc = NULL WHERE sloc = -1 OR sloc = '-1'\")\n",
        "conn.commit()\n",
        "cursor.execute(\"UPDATE package_data SET total_lines_of_code = NULL WHERE total_lines_of_code = -1 OR total_lines_of_code = '-1'\")\n",
        "conn.commit()\n",
        "cursor.execute(\"UPDATE package_data SET no_of_files = NULL WHERE no_of_files = -1 OR no_of_files = '-1'\")\n",
        "conn.commit()"
      ],
      "metadata": {
        "id": "ksBlTVTN3_LT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GTVOt-RGkoL"
      },
      "outputs": [],
      "source": [
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Renaming some columns"
      ],
      "metadata": {
        "id": "xbE6IoBp2bAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cursor.execute(f\"ALTER TABLE package_data RENAME COLUMN filtered_lines_of_code TO sloc\")\n",
        "conn.commit()"
      ],
      "metadata": {
        "id": "P5jDlwCh2kTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding a new metric derived from `last_modified_timestamp` in days"
      ],
      "metadata": {
        "id": "THC7KEg0CTRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['last_modified_timestamp'] = pd.to_datetime(df['last_modified_timestamp'])\n",
        "\n",
        "# comparing to 23 Feb, 2023\n",
        "comparison_date = pd.to_datetime('2024-02-23')\n",
        "\n",
        "df['last_modified_timestamp'] = df['last_modified_timestamp'].dt.tz_localize(None)\n",
        "df['days_since_last_modification'] = (comparison_date - df['last_modified_timestamp']).dt.days\n",
        "\n",
        "# alter_query = \"ALTER TABLE package_data ADD COLUMN days_since_last_modification INTEGER;\"\n",
        "# conn.execute(alter_query)\n",
        "\n",
        "update_query = \"\"\"\n",
        "    UPDATE package_data\n",
        "    SET days_since_last_modification = ?\n",
        "    WHERE package = ?\n",
        "\"\"\"\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "  values = (row['days_since_last_modification'], row['package'])\n",
        "  # print(\"Updating package:\", row['package'])\n",
        "  conn.execute(update_query, values)\n",
        "\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "nWRLErZ6EYMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting text values into numerical values"
      ],
      "metadata": {
        "id": "IEdV9AxL2VxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cursor.execute(\"ALTER TABLE package_data ADD COLUMN security_numeric INTEGER\")\n",
        "# conn.commit()\n",
        "\n",
        "security_update_query = \"\"\"\n",
        "UPDATE package_data\n",
        "SET security_numeric =\n",
        "    CASE\n",
        "        WHEN security = 'SECURITY REVIEW NEEDED' THEN 50\n",
        "        WHEN security = 'NO KNOWN SECURITY ISSUES' THEN 100\n",
        "        WHEN security = '?' THEN NULL\n",
        "        WHEN security = 'SECURITY ISSUES FOUND' THEN 10\n",
        "        ELSE NULL\n",
        "    END\n",
        "\"\"\"\n",
        "cursor.execute(security_update_query)\n",
        "conn.commit()"
      ],
      "metadata": {
        "id": "Fnp4bKif2ewk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cursor.execute(\"ALTER TABLE package_data ADD COLUMN popularity_numeric INTEGER\")\n",
        "# conn.commit()\n",
        "\n",
        "popularity_update_query = \"\"\"\n",
        "UPDATE package_data\n",
        "SET popularity_numeric =\n",
        "    CASE\n",
        "        WHEN popularity = 'LIMITED' THEN 20\n",
        "        WHEN popularity = 'INFLUENTIAL PROJECT' THEN 100\n",
        "        WHEN popularity = 'SMALL' THEN 10\n",
        "        WHEN popularity = 'RECOGNIZED' THEN 60\n",
        "        WHEN popularity = 'POPULAR' THEN 50\n",
        "        WHEN popularity = 'KEY ECOSYSTEM PROJECT' THEN 80\n",
        "        ELSE NULL\n",
        "    END\n",
        "\"\"\"\n",
        "cursor.execute(popularity_update_query)\n",
        "conn.commit()"
      ],
      "metadata": {
        "id": "GWK2jzkh-uDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cursor.execute(\"ALTER TABLE package_data ADD COLUMN maintenance_numeric INTEGER\")\n",
        "# conn.commit()\n",
        "\n",
        "maintenance_update_query = \"\"\"\n",
        "UPDATE package_data\n",
        "SET maintenance_numeric =\n",
        "    CASE\n",
        "        WHEN maintenance = 'INACTIVE' THEN 5\n",
        "        WHEN maintenance = 'SUSTAINABLE' THEN 50\n",
        "        WHEN maintenance = 'HEALTHY' THEN 100\n",
        "        ELSE NULL\n",
        "    END\n",
        "\"\"\"\n",
        "cursor.execute(maintenance_update_query)\n",
        "conn.commit()"
      ],
      "metadata": {
        "id": "tzFL_O9A_CKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cursor.execute(\"ALTER TABLE package_data ADD COLUMN community_numeric INTEGER\")\n",
        "# conn.commit()\n",
        "\n",
        "community_update_query = \"\"\"\n",
        "UPDATE package_data\n",
        "SET community_numeric =\n",
        "    CASE\n",
        "        WHEN community = 'LIMITED' THEN 20\n",
        "        WHEN community = 'SUSTAINABLE' THEN 50\n",
        "        WHEN community = 'ACTIVE' THEN 100\n",
        "        ELSE NULL\n",
        "    END\n",
        "\"\"\"\n",
        "cursor.execute(community_update_query)\n",
        "conn.commit()"
      ],
      "metadata": {
        "id": "mrd8tgecAR3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accounting for cases when health score is zero\n",
        "maintenance_update_query = \"\"\"\n",
        "UPDATE package_data\n",
        "SET security_numeric = 0, popularity_numeric = 0, community_numeric = 0, maintenance_numeric = 0 WHERE health_score = 0\n",
        "\"\"\"\n",
        "cursor.execute(maintenance_update_query)\n",
        "conn.commit()"
      ],
      "metadata": {
        "id": "bLpBJMgJDF4b"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "dAbVoPDmJKrR",
        "5TkJKg7MJOpQ",
        "7kyxTPar_4kd",
        "knfu0leNEaxY",
        "PQkSHf9cLU_E"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}